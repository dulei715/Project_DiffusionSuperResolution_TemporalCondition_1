\section{Experiments}
\label{sec:experiments}

To comprehensively evaluate the effectiveness of the proposed Multi-Resolution Conditional Diffusion Model (MR-CDM), we conduct extensive experiments on multiple real-world time series forecasting benchmarks. This section details the experimental setup, datasets, baseline comparisons, and presents both quantitative and qualitative analysis of the results.

\subsection{Experimental Setup}
\label{subsec:setup}

\subsubsection{Implementation Details}
Our MR-CDM is implemented in PyTorch 1.13.0 with Python 3.9. All experiments are conducted on a server with 4Ã— NVIDIA A100 GPUs (40GB memory each). We employ the AdamW optimizer with an initial learning rate of $1\times10^{-4}$ and weight decay of $1\times10^{-5}$. The model is trained for 500 epochs with a batch size of 32. We use a cosine annealing learning rate scheduler with warm-up for the first 50 epochs.

The diffusion process employs a linear noise schedule with $\beta_{\text{min}} = 1\times10^{-4}$ and $\beta_{\text{max}} = 0.02$ over $T=1000$ diffusion steps. For the multi-resolution decomposition, we set $S=4$ levels with pooling kernel sizes of $[2, 3, 4, 6]$ respectively. The hidden dimension $d$ is set to 64 for all resolution levels.

\subsubsection{Evaluation Metrics}
We adopt three widely-used metrics for time series forecasting evaluation:

\begin{itemize}
    \item \textbf{Mean Absolute Error (MAE)}: 
    $\text{MAE} = \frac{1}{H}\sum_{i=1}^{H}|\hat{y}_i - y_i|$
    
    \item \textbf{Root Mean Square Error (RMSE)}: 
    $\text{RMSE} = \sqrt{\frac{1}{H}\sum_{i=1}^{H}(\hat{y}_i - y_i)^2}$
    
    \item \textbf{Mean Absolute Percentage Error (MAPE)}: 
    $\text{MAPE} = \frac{100\%}{H}\sum_{i=1}^{H}\left|\frac{\hat{y}_i - y_i}{y_i}\right|$
\end{itemize}

where $H$ is the forecasting horizon, $\hat{y}_i$ is the predicted value, and $y_i$ is the ground truth.

\subsubsection{Baseline Methods}
We compare MR-CDM against state-of-the-art time series forecasting methods:

\begin{itemize}
    \item \textbf{Traditional Methods}: ARIMA, Prophet
    \item \textbf{Deep Learning Methods}: 
    \begin{itemize}
        \item \textbf{TCN} \cite{2018Trellis}: Temporal Convolutional Networks
        \item \textbf{LSTM} \cite{6795963}: Long Short-Term Memory
        \item \textbf{Transformer} \cite{2017Attention}: Vanilla Transformer
        \item \textbf{Informer} \cite{2020Informer}: Transformer-based for long sequence forecasting
        \item \textbf{Autoformer} \cite{2021Autoformer}: Auto-correlation mechanism
        \item \textbf{TimeGrad} \cite{2021Autoregressive}: Diffusion-based time series model
        \item \textbf{CSDI} \cite{2021CSDI}: Conditional Score-based Diffusion model
    \end{itemize}
\end{itemize}

All baselines are implemented using their official codebases and tuned to optimal performance on each dataset.

\subsection{Datasets}
\label{subsec:datasets}

We evaluate on four diverse time series datasets covering different domains and characteristics:

\begin{table}[!t]
\centering
\caption{Summary of benchmark datasets}
\label{tab:datasets}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Length} & \textbf{Features} & \textbf{Frequency} & \textbf{Forecasting Horizon} \\
\midrule
Electricity (ECL) & 26,304 & 321 & Hourly & 96 (24 hours) \\
Traffic & 17,544 & 207 & 5-min & 288 (24 hours) \\
Weather & 52,696 & 21 & 10-min & 144 (24 hours) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Electricity Load Consumption (ECL)}
The ECL dataset contains hourly electricity consumption of 321 clients from 2012 to 2014. We use the first 80\% for training, 10\% for validation, and the remaining 10\% for testing. The task is to predict the next 24 hours (96 time steps) of consumption.

\subsubsection{Traffic}
Traffic is a collection of hourly data from California Department of Transportation, which describes the road occupancy rates measured by different sensors on San Francisco Bay area freeways.

\subsubsection{Weather}
This dataset includes 21 meteorological indicators (temperature, humidity, wind speed, etc.) recorded every 10 minutes for 2020. The forecasting horizon is set to 24 hours (144 time steps).

All datasets are normalized using Z-score normalization: $x_{\text{norm}} = (x - \mu)/\sigma$, where $\mu$ and $\sigma$ are computed from the training set.

\subsection{Main Results}
\label{subsec:results}

\subsubsection{Quantitative Comparison}

Table \ref{tab:main_results} presents the comprehensive comparison between MR-CDM and baseline methods across all datasets and forecasting horizons.

\begin{table*}[!t]
\centering
\caption{Overall performance comparison on four datasets. Best results are in \textbf{bold}, second best are \underline{underlined}.}
\label{tab:main_results}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{ECL}} & \multicolumn{2}{c}{\textbf{Traffic}} & \multicolumn{2}{c}{\textbf{Weather}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} 
 & MAE & RMSE & MAE & RMSE & MAE & RMSE \\
\midrule
ARIMA & 0.312 & 0.415 & 0.285 & 0.398 & 0.198 & 0.265 \\
Prophet & 0.298 & 0.402 & 0.276 & 0.385 & 0.187 & 0.251 \\
TCN & 0.274 & 0.378 & 0.258 & 0.362 & 0.172 & 0.238 \\
LSTM & 0.268 & 0.369 & 0.251 & 0.354 & 0.165 & 0.229 \\
Transformer & 0.256 & 0.358 & 0.243 & 0.342 & 0.158 & 0.221 \\
Informer & 0.242 & 0.341 & 0.231 & 0.328 & 0.149 & 0.212 \\
Autoformer & 0.235 & 0.332 & 0.225 & 0.319 & 0.142 & 0.205 \\
TimeGrad & 0.228 & 0.324 & 0.218 & 0.312 & 0.136 & 0.198 \\
CSDI & 0.221 & 0.317 & 0.212 & 0.305 & 0.131 & 0.192  \\
\midrule
\textbf{MR-CDM (Ours)} & \textbf{0.205} & \textbf{0.298} & \textbf{0.198} & \textbf{0.287} & \textbf{0.118} & \textbf{0.175}  \\
\textbf{Improvement} & \textbf{7.2\%} & \textbf{6.0\%} & \textbf{6.6\%} & \textbf{5.9\%} & \textbf{9.9\%} & \textbf{8.9\%}  \\
\bottomrule
\end{tabular}
\end{table*}

Our MR-CDM achieves state-of-the-art performance across all datasets and metrics. Specifically, it reduces MAE by 6.0-9.9\% compared to the strongest baseline (CSDI). The improvement is particularly significant on the Weather and Solar datasets, demonstrating the advantage of multi-resolution modeling for complex meteorological patterns.



\subsection{Ablation Studies}
\label{subsec:ablation}

We conduct ablation studies to validate the effectiveness of key components in MR-CDM:

\begin{table}[!t]
\centering
\caption{Ablation study on ECL dataset (MAE/RMSE)}
\label{tab:ablation}
\begin{tabular}{lc}
\toprule
\textbf{Variant} & \textbf{ECL (MAE/RMSE)} \\
\midrule
Full MR-CDM & \textbf{0.205} / \textbf{0.298} \\
\hline
w/o Multi-Resolution & 0.231 / 0.327 \\
w/o Conditional Guidance & 0.242 / 0.339 \\
w/o Diffusion Process & 0.225 / 0.318 \\
Single Resolution Only & 0.218 / 0.309 \\
Fixed-Scale Input & 0.228 / 0.321 \\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:ablation} shows that removing any major component leads to performance degradation. The multi-resolution mechanism contributes most significantly (11.2\% MAE drop when removed), confirming its importance for capturing temporal patterns at different scales.

\subsection{Computational Efficiency}
\label{subsec:efficiency}

\begin{table}[!t]
\centering
\caption{Computational cost comparison (ECL dataset)}
\label{tab:efficiency}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Params (M)} & \textbf{Training Time (h)} & \textbf{Inference Time (ms)} \\
\midrule
Informer & 14.3 & 3.2 & 12.4 \\
Autoformer & 16.8 & 3.8 & 14.2 \\
TimeGrad & 18.2 & 4.5 & 18.7 \\
CSDI & 19.1 & 5.1 & 21.3 \\
\textbf{MR-CDM} & \textbf{22.4} & \textbf{5.8} & \textbf{25.6} \\
\bottomrule
\end{tabular}

\end{table}

While MR-CDM has higher computational cost due to the iterative diffusion process (Table \ref{tab:efficiency}), the performance improvement justifies the additional resources. The multi-resolution design enables parallel processing across scales, mitigating the overhead of the diffusion mechanism.

\subsection{Discussion}
\label{subsec:discussion}

The experimental results demonstrate that MR-CDM consistently outperforms existing methods across diverse time series forecasting tasks. The key advantages include:

\begin{itemize}
    \item \textbf{Multi-scale Modeling}: The hierarchical decomposition effectively captures patterns at different temporal resolutions
    \item \textbf{Probabilistic Forecasting}: The diffusion process generates diverse and realistic future trajectories
    \item \textbf{Scale Flexibility}: The delay embedding enables handling of variable-length inputs without retraining
    \item \textbf{Conditional Generation}: The guidance mechanism incorporates both historical context and coarse-scale information
\end{itemize}

However, the iterative nature of diffusion models increases inference time compared to single-pass models. Future work will focus on developing faster sampling strategies and adaptive resolution selection to further improve efficiency.
