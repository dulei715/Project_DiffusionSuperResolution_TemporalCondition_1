@article{2013ARIMA,
	title={ARIMA: The Models of Box and Jenkins},
	author={ Stellwagen, E.  and  Tashman, L. },
	journal={Foresight: The International Journal of Applied Forecasting},
	pages={28-33},
	year={2013},
}
@Inbook{Zivot2003,
	author="Zivot, Eric
	and Wang, Jiahui",
	title="Vector Autoregressive Models for Multivariate Time Series",
	bookTitle="Modeling Financial Time Series with S-Plus®",
	year="2003",
	publisher="Springer New York",
	address="New York, NY",
	pages="369--413",
	abstract="The vector autoregression (VAR) model is one of the most successful, flexible, and easy to use models for the analysis of multivariate time series. It is a natural extension of the univariate autoregressive model to dynamic multivariate time series. The VAR model has proven to be especially useful for describing the dynamic behavior of economic and financial time series and for forecasting. It often provides superior forecasts to those from univariate time series models and elaborate theory-based simultaneous equations models. Forecasts from VAR models are quite flexible because they can be made conditional on the potential future paths of specified variables in the model.",
	isbn="978-0-387-21763-5",
	doi="10.1007/978-0-387-21763-5_11",
	url="https://doi.org/10.1007/978-0-387-21763-5_11"
}
@article{KOEHLER2001269,
	title = {Forecasting models and prediction intervals for the multiplicative Holt–Winters method},
	journal = {International Journal of Forecasting},
	volume = {17},
	number = {2},
	pages = {269-286},
	year = {2001},
	issn = {0169-2070},
	doi = {https://doi.org/10.1016/S0169-2070(01)00081-4},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207001000814},
	author = {Anne B. Koehler and Ralph D. Snyder and J.Keith Ord},
	keywords = {Holt–Winters method, Exponential smoothing, State space models, Prediction intervals, Model selection},
	abstract = {A new class of models for data showing trend and multiplicative seasonality is presented. The models allow the forecast error variance to depend on the trend and/or the seasonality. It is shown that each of these models has essentially the same updating equations and forecast functions as the multiplicative Holt–Winters method, whether or not the error variation in the model is constant. Although the different models produce identical updating relationships for the point forecast, the prediction intervals, of course, depend on the structure of the error variance and so it is essential to be able to choose the most appropriate form of model. Two methods for model selection are presented and examined by simulation. For the most common case of series with an upward trend, we recommend using a model with variance dependent on both the trend and seasonal elements.}
}
@ARTICLE{RePEc:bla:istatr:v:77:y:2009:i:2:p:315-316,
	title = {Forecasting with Exponential Smoothing: The State Space Approach by Rob J. Hyndman, Anne B. Koehler, J. Keith Ord, Ralph D. Snyder},
	author = {Hand, David J.},
	year = {2009},
	journal = {International Statistical Review},
	volume = {77},
	number = {2},
	pages = {315-316},
	url = {https://EconPapers.repec.org/RePEc:bla:istatr:v:77:y:2009:i:2:p:315-316}
}
@article{2005Gaussian,
	title={Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)},
	author={ Rasmussen, Carl Edward  and  Williams, Christopher K I },
	journal={The MIT Press},
	year={2005},
}
@ARTICLE{6795963,
	author={Hochreiter, Sepp and Schmidhuber, Jürgen},
	journal={Neural Computation}, 
	title={Long Short-Term Memory}, 
	year={1997},
	volume={9},
	number={8},
	pages={1735-1780},
	keywords={},
	doi={10.1162/neco.1997.9.8.1735}
}
@article{2014Learning,
	title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
	author={ Cho, Kyunghyun  and  Van Merrienboer, Bart  and  Gulcehre, Caglar  and  Bahdanau, Dzmitry  and  Bougares, Fethi  and  Schwenk, Holger  and  Bengio, Yoshua },
	journal={Computer Science},
	year={2014},
}
@article{2020DeepAR,
	title={DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks},
	author={ Flunkert, Valentin  and  Salinas, David  and  Gasthaus, Jan },
	journal={International Journal of Forecasting},
	volume={36},
	number={3},
	year={2020},
}
@article{2016WaveNet,
	title={WaveNet: A Generative Model for Raw Audio},
	author={ Oord, Aaron Van Den  and  Dieleman, Sander  and  Zen, Heiga  and  Simonyan, Karen  and  Kavukcuoglu, Koray },
	year={2016},
}
@article{2018Trellis,
	title={Trellis Networks for Sequence Modeling},
	author={ Bai, Shaojie  and  Kolter, J. Zico  and  Koltun, Vladlen },
	year={2018},
}
@article{2017Attention,
	title={Attention Is All You Need},
	author={ Vaswani, Ashish  and  Shazeer, Noam  and  Parmar, Niki  and  Uszkoreit, Jakob  and  Jones, Llion  and  Gomez, Aidan N  and  Kaiser, Lukasz  and  Polosukhin, Illia },
	journal={arXiv},
	year={2017},
}
@article{2020Informer,
	title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
	author={ Zhou, Haoyi  and  Zhang, Shanghang  and  Peng, Jieqi  and  Zhang, Shuai  and  Zhang, Wancai },
	year={2020},
}
@article{2021Autoformer,
	title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},
	author={ Wu, Haixu  and  Xu, Jiehui  and  Wang, Jianmin  and  Long, Mingsheng },
	year={2021},
}
@article{2015Deep,
	title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
	author={ Sohl-Dickstein, Jascha  and  Weiss, Eric A  and  Maheswaranathan, Niru  and  Ganguli, Surya },
	journal={JMLR.org},
	year={2015},
}
@article{2021Autoregressive,
	title={Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting},
	author={ Rasul, Kashif  and  Seward, Calvin  and  Schuster, Ingmar  and  Vollgraf, Roland },
	year={2021},
}
@article{2021CSDI,
	title={CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation},
	author={ Tashiro, Yusuke  and  Song, Jiaming  and  Song, Yang  and  Ermon, Stefano },
	year={2021},
}
@article{alcaraz2022diffusion,
	title={Diffusion-based time series imputation and forecasting with structured state space models},
	author={Alcaraz, Juan Miguel Lopez and Strodthoff, Nils},
	journal={Transactions on Machine Learning Research},
	year={2022},
	issn={2835-8856},
	url={https://openreview.net/forum?id=hHlBk7ApW},
	archiveprefix={arXiv},
	eprint={2208.09399},
	primaryclass={cs.LG},
	note={Featured Certification}
}
@article{naiman_utilizing_nodate,
	title = {Utilizing {Image} {Transforms} and {Diffusion} {Models} for {Generative} {Modeling} of {Short} and {Long} {Time} {Series}},
	abstract = {Lately, there has been a surge in interest surrounding generative modeling of time series data. Most existing approaches are designed either to process short sequences or to handle long-range sequences. This dichotomy can be attributed to gradient issues with recurrent networks, computational costs associated with transformers, and limited expressiveness of state space models. Towards a unified generative model for varying-length time series, we propose in this work to transform sequences into images. By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature. We validate the effectiveness of our method through a comprehensive evaluation across multiple tasks, including unconditional generation, interpolation, and extrapolation. We show that our approach achieves consistently state-of-the-art results against strong baselines. In the unconditional generation tasks, we show remarkable mean improvements of 58.17\% over previous diffusion models in the short discriminative score and 132.61\% in the (ultra-)long classification scores. Code is at https://github.com/azencot-group/ImagenTime.},
	language = {en},
	author = {Naiman, Ilan and Berman, Nimrod},
	file = {PDF:files/79/Naiman和Berman - Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Serie.pdf:application/pdf},
}
@inproceedings{10.1145/3589132.3625614,
	author = {Wen, Haomin and Lin, Youfang and Xia, Yutong and Wan, Huaiyu and Wen, Qingsong and Zimmermann, Roger and Liang, Yuxuan},
	title = {DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models},
	year = {2023},
	isbn = {9798400701689},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3589132.3625614},
	doi = {10.1145/3589132.3625614},
	abstract = {Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular de-noising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4\%-14\%, and Root Mean Squared Error (RMSE) by 2\%-7\% over existing methods on three real-world datasets.},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	articleno = {60},
	numpages = {12},
	keywords = {spatio-temporal graph forecasting, probabilistic forecasting, diffusion model},
	location = {Hamburg, Germany},
	series = {SIGSPATIAL '23}
}
@article{shen_multi-resolution_2024,
	title = {{MULTI}-{RESOLUTION} {DIFFUSION} {MODELS} {FOR} {TIME} {SERIES} {FORECASTING}},
	abstract = {The diffusion model has been successfully used in many computer vision applications, such as text-guided image generation and image-to-image translation. Recently, there have been attempts on extending the diffusion model for time series data. However, these extensions are fairly straightforward and do not utilize the unique properties of time series data. As different patterns are usually exhibited at multiple scales of a time series, we in this paper leverage this multi-resolution temporal structure and propose the multi-resolution diffusion model (mr-Diff). By using the seasonal-trend decomposition, we sequentially extract fine-to-coarse trends from the time series for forward diffusion. The denoising process then proceeds in an easy-to-hard non-autoregressive manner. The coarsest trend is generated first. Finer details are progressively added, using the predicted coarser trends as condition variables. Experimental results on nine real-world time series datasets demonstrate that mr-Diff outperforms state-of-the-art time series diffusion models. It is also better than or comparable across a wide variety of advanced time series prediction models.},
	language = {en},
	author = {Shen, Lifeng and Chen, Weiyu and Kwok, James T},
	year = {2024},
	file = {PDF:files/77/Shen 等 - 2024 - MULTI-RESOLUTION DIFFUSION MODELS FOR TIME SERIES FORECASTING.pdf:application/pdf},
}
